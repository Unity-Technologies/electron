From 0000000000000000000000000000000000000000 Thu Apr 02 10:38:12 2020
From: Tautvydas Å½ilys <tautvydas.zilys@gmail.com>
Date: Thu, 02 Apr 2020 10:38:12 -0700
Subject: Fix page size for running on Apple silicon

diff --git a/base/allocator/partition_allocator/page_allocator_constants.h b/base/allocator/partition_allocator/page_allocator_constants.h
index ac82360440f3..8e5d2d433c44 100644
--- a/base/allocator/partition_allocator/page_allocator_constants.h
+++ b/base/allocator/partition_allocator/page_allocator_constants.h
@@ -12,7 +12,7 @@
 namespace base {
 #if defined(OS_WIN)
 static constexpr size_t kPageAllocationGranularityShift = 16;  // 64KB
-#elif defined(_MIPS_ARCH_LOONGSON)
+#elif defined(_MIPS_ARCH_LOONGSON) || defined(OS_MACOSX)
 static constexpr size_t kPageAllocationGranularityShift = 14;  // 16KB
 #else
 static constexpr size_t kPageAllocationGranularityShift = 12;  // 4KB
@@ -24,7 +24,8 @@ static constexpr size_t kPageAllocationGranularityOffsetMask =
 static constexpr size_t kPageAllocationGranularityBaseMask =
     ~kPageAllocationGranularityOffsetMask;
 
-#if defined(_MIPS_ARCH_LOONGSON)
+// Support for 16k page macOS
+#if defined(_MIPS_ARCH_LOONGSON) || defined(OS_MACOSX)
 static constexpr size_t kSystemPageSize = 16384;
 #else
 static constexpr size_t kSystemPageSize = 4096;
diff --git a/base/allocator/partition_allocator/partition_alloc_constants.h b/base/allocator/partition_allocator/partition_alloc_constants.h
index 474405f0ddb9..69ab778a13ff 100644
--- a/base/allocator/partition_allocator/partition_alloc_constants.h
+++ b/base/allocator/partition_allocator/partition_alloc_constants.h
@@ -33,7 +33,7 @@ static const size_t kBucketShift = (kAllocationGranularity == 8) ? 3 : 2;
 // other constant values, we pack _all_ `PartitionRootGeneric::Alloc` sizes
 // perfectly up against the end of a system page.
 
-#if defined(_MIPS_ARCH_LOONGSON)
+#if defined(_MIPS_ARCH_LOONGSON) || defined(OS_MACOSX)
 static const size_t kPartitionPageShift = 16;  // 64 KiB
 #else
 static const size_t kPartitionPageShift = 14;  // 16 KiB
diff --git a/base/mac/scoped_mach_vm.cc b/base/mac/scoped_mach_vm.cc
index d52c77f63986..c52aea22ec00 100644
--- a/base/mac/scoped_mach_vm.cc
+++ b/base/mac/scoped_mach_vm.cc
@@ -8,8 +8,8 @@ namespace base {
 namespace mac {
 
 void ScopedMachVM::reset(vm_address_t address, vm_size_t size) {
-  DCHECK_EQ(address % PAGE_SIZE, 0u);
-  DCHECK_EQ(size % PAGE_SIZE, 0u);
+  DCHECK_EQ(address % getpagesize(), 0u);
+  DCHECK_EQ(size % getpagesize(), 0u);
 
   if (size_) {
     if (address_ < address) {
diff --git a/base/mac/scoped_mach_vm.h b/base/mac/scoped_mach_vm.h
index 58a13f664973..44b04d75568b 100644
--- a/base/mac/scoped_mach_vm.h
+++ b/base/mac/scoped_mach_vm.h
@@ -7,6 +7,7 @@
 
 #include <mach/mach.h>
 #include <stddef.h>
+#include <unistd.h>
 
 #include <algorithm>
 
@@ -50,8 +51,8 @@ class BASE_EXPORT ScopedMachVM {
  public:
   explicit ScopedMachVM(vm_address_t address = 0, vm_size_t size = 0)
       : address_(address), size_(size) {
-    DCHECK_EQ(address % PAGE_SIZE, 0u);
-    DCHECK_EQ(size % PAGE_SIZE, 0u);
+    DCHECK_EQ(address % getpagesize(), 0u);
+    DCHECK_EQ(size % getpagesize(), 0u);
   }
 
   ~ScopedMachVM() {
diff --git a/base/process/process_metrics_ios.cc b/base/process/process_metrics_ios.cc
index 83fc3d69331b..686c627283c8 100644
--- a/base/process/process_metrics_ios.cc
+++ b/base/process/process_metrics_ios.cc
@@ -83,16 +83,16 @@ bool GetSystemMemoryInfo(SystemMemoryInfoKB* meminfo) {
   }
   DCHECK_EQ(HOST_VM_INFO64_COUNT, count);
 
-  // Check that PAGE_SIZE is divisible by 1024 (2^10).
-  CHECK_EQ(PAGE_SIZE, (PAGE_SIZE >> 10) << 10);
+  // Check that getpagesize() is divisible by 1024 (2^10).
+  CHECK_EQ(getpagesize(), (getpagesize() >> 10) << 10);
   meminfo->free = saturated_cast<int>(
-      PAGE_SIZE / 1024 * (vm_info.free_count - vm_info.speculative_count));
+      getpagesize() / 1024 * (vm_info.free_count - vm_info.speculative_count));
   meminfo->speculative =
-      saturated_cast<int>(PAGE_SIZE / 1024 * vm_info.speculative_count);
+      saturated_cast<int>(getpagesize() / 1024 * vm_info.speculative_count);
   meminfo->file_backed =
-      saturated_cast<int>(PAGE_SIZE / 1024 * vm_info.external_page_count);
+      saturated_cast<int>(getpagesize() / 1024 * vm_info.external_page_count);
   meminfo->purgeable =
-      saturated_cast<int>(PAGE_SIZE / 1024 * vm_info.purgeable_count);
+      saturated_cast<int>(getpagesize() / 1024 * vm_info.purgeable_count);
 
   return true;
 }
diff --git a/base/process/process_metrics_mac.cc b/base/process/process_metrics_mac.cc
index cbb5e937a9e9..b5c1d7d48a90 100644
--- a/base/process/process_metrics_mac.cc
+++ b/base/process/process_metrics_mac.cc
@@ -255,7 +255,7 @@ size_t GetSystemCommitCharge() {
     return 0;
   }
 
-  return (data.active_count * PAGE_SIZE) / 1024;
+  return (data.active_count * getpagesize()) / 1024;
 }
 
 bool GetSystemMemoryInfo(SystemMemoryInfoKB* meminfo) {
@@ -280,15 +280,15 @@ bool GetSystemMemoryInfo(SystemMemoryInfoKB* meminfo) {
   }
   DCHECK_EQ(HOST_VM_INFO64_COUNT, count);
 
-  static_assert(PAGE_SIZE % 1024 == 0, "Invalid page size");
+  DCHECK(getpagesize() % 1024 == 0);
   meminfo->free = saturated_cast<int>(
-      PAGE_SIZE / 1024 * (vm_info.free_count - vm_info.speculative_count));
+      getpagesize() / 1024 * (vm_info.free_count - vm_info.speculative_count));
   meminfo->speculative =
-      saturated_cast<int>(PAGE_SIZE / 1024 * vm_info.speculative_count);
+      saturated_cast<int>(getpagesize() / 1024 * vm_info.speculative_count);
   meminfo->file_backed =
-      saturated_cast<int>(PAGE_SIZE / 1024 * vm_info.external_page_count);
+      saturated_cast<int>(getpagesize() / 1024 * vm_info.external_page_count);
   meminfo->purgeable =
-      saturated_cast<int>(PAGE_SIZE / 1024 * vm_info.purgeable_count);
+      saturated_cast<int>(getpagesize() / 1024 * vm_info.purgeable_count);
 
   return true;
 }
diff --git a/base/trace_event/process_memory_dump.cc b/base/trace_event/process_memory_dump.cc
index 5a049cc1d5c8..fd46cbf818f6 100644
--- a/base/trace_event/process_memory_dump.cc
+++ b/base/trace_event/process_memory_dump.cc
@@ -211,16 +211,16 @@ base::Optional<size_t> ProcessMemoryDump::CountResidentBytesInSharedMemory(
   // Sanity check in case the mapped size is less than the total size of the
   // region.
   size_t pages_to_fault =
-      std::min(resident_pages, (mapped_size + PAGE_SIZE - 1) / PAGE_SIZE);
+      std::min(resident_pages, (mapped_size + getpagesize() - 1) / getpagesize());
 
   volatile char* base_address = static_cast<char*>(start_address);
   for (size_t i = 0; i < pages_to_fault; ++i) {
     // Reading from a volatile is a visible side-effect for the purposes of
     // optimization. This guarantees that the optimizer will not kill this line.
-    base_address[i * PAGE_SIZE];
+    base_address[i * getpagesize()];
   }
 
-  return resident_pages * PAGE_SIZE;
+  return resident_pages * getpagesize();
 #else
   return CountResidentBytes(start_address, mapped_size);
 #endif  // defined(OS_MACOSX) && !defined(OS_IOS)
diff --git a/build/config/mac/BUILD.gn b/build/config/mac/BUILD.gn
index 780f7520607c..7364b36c5933 100644
--- a/build/config/mac/BUILD.gn
+++ b/build/config/mac/BUILD.gn
@@ -47,6 +47,7 @@ config("compiler") {
     "PROTECTED_MEMORY",
     "rw",
     "r",
+    "-Wl,-segalign,0x4000",
   ]
 
   if (save_unstripped_output) {
diff --git a/third_party/crashpad/crashpad/snapshot/fuchsia/process_snapshot_fuchsia_test.cc b/third_party/crashpad/crashpad/snapshot/fuchsia/process_snapshot_fuchsia_test.cc
index a96d37c03eb6..4cb6b3bc954e 100644
--- a/third_party/crashpad/crashpad/snapshot/fuchsia/process_snapshot_fuchsia_test.cc
+++ b/third_party/crashpad/crashpad/snapshot/fuchsia/process_snapshot_fuchsia_test.cc
@@ -55,7 +55,7 @@ CRASHPAD_CHILD_TEST_MAIN(AddressSpaceChildTestMain) {
   // correctly.
   for (const auto& t : kTestMappingPermAndSizes) {
     zx_handle_t vmo = ZX_HANDLE_INVALID;
-    const size_t size = t.pages * PAGE_SIZE;
+    const size_t size = t.pages * getpagesize();
     zx_status_t status = zx_vmo_create(size, 0, &vmo);
     ZX_CHECK(status == ZX_OK, status) << "zx_vmo_create";
     uintptr_t mapping_addr = 0;
@@ -119,7 +119,7 @@ class AddressSpaceTest : public MultiprocessExec {
       const auto& t = kTestMappingPermAndSizes[i];
       EXPECT_TRUE(HasSingleMatchingMapping(process_snapshot.MemoryMap(),
                                            test_addresses[i],
-                                           t.pages * PAGE_SIZE,
+                                           t.pages * getpagesize(),
                                            t.minidump_perm))
           << base::StringPrintf(
                  "index %zu, zircon_perm 0x%x, minidump_perm 0x%x",
diff --git a/third_party/crashpad/crashpad/util/process/process_memory_mac.cc b/third_party/crashpad/crashpad/util/process/process_memory_mac.cc
index eba1f2016477..da2ce5bfed04 100644
--- a/third_party/crashpad/crashpad/util/process/process_memory_mac.cc
+++ b/third_party/crashpad/crashpad/util/process/process_memory_mac.cc
@@ -110,7 +110,7 @@ ssize_t ProcessMemoryMac::ReadUpTo(VMAddress address,
   if (!memory) {
     // If we can not read the entire mapping, try to perform a short read of the
     // first page instead. This is necessary to support ReadCString().
-    size_t short_read = PAGE_SIZE - (address % PAGE_SIZE);
+    size_t short_read = getpagesize() - (address % getpagesize());
     if (short_read >= size)
       return -1;
 
diff --git a/third_party/crashpad/crashpad/util/process/process_memory_mac_test.cc b/third_party/crashpad/crashpad/util/process/process_memory_mac_test.cc
index d801bb149fed..159f5560482c 100644
--- a/third_party/crashpad/crashpad/util/process/process_memory_mac_test.cc
+++ b/third_party/crashpad/crashpad/util/process/process_memory_mac_test.cc
@@ -33,7 +33,7 @@ namespace {
 
 TEST(ProcessMemoryMac, ReadMappedSelf) {
   vm_address_t address = 0;
-  constexpr vm_size_t kSize = 4 * PAGE_SIZE;
+  constexpr vm_size_t kSize = 4 * getpagesize();
   kern_return_t kr =
       vm_allocate(mach_task_self(), &address, kSize, VM_FLAGS_ANYWHERE);
   ASSERT_EQ(kr, KERN_SUCCESS) << MachErrorMessage(kr, "vm_allocate");
@@ -73,8 +73,8 @@ TEST(ProcessMemoryMac, ReadMappedSelf) {
   EXPECT_EQ(memcmp(region + 1, mapped->data(), kSize - 2), 0);
 
   // Ensure that a read of exactly one page works.
-  ASSERT_TRUE((mapped = memory.ReadMapped(address + PAGE_SIZE, PAGE_SIZE)));
-  EXPECT_EQ(memcmp(region + PAGE_SIZE, mapped->data(), PAGE_SIZE), 0);
+  ASSERT_TRUE((mapped = memory.ReadMapped(address + getpagesize(), getpagesize())));
+  EXPECT_EQ(memcmp(region + getpagesize(), mapped->data(), getpagesize()), 0);
 
   // Ensure that a read of a single byte works.
   ASSERT_TRUE((mapped = memory.ReadMapped(address + 2, 1)));
@@ -88,7 +88,7 @@ TEST(ProcessMemoryMac, ReadMappedSelf) {
 
 TEST(ProcessMemoryMac, ReadSelfUnmapped) {
   vm_address_t address = 0;
-  constexpr vm_size_t kSize = 2 * PAGE_SIZE;
+  constexpr vm_size_t kSize = 2 * getpagesize();
   kern_return_t kr =
       vm_allocate(mach_task_self(), &address, kSize, VM_FLAGS_ANYWHERE);
   ASSERT_EQ(kr, KERN_SUCCESS) << MachErrorMessage(kr, "vm_allocate");
@@ -102,7 +102,7 @@ TEST(ProcessMemoryMac, ReadSelfUnmapped) {
   }
 
   kr = vm_protect(
-      mach_task_self(), address + PAGE_SIZE, PAGE_SIZE, FALSE, VM_PROT_NONE);
+      mach_task_self(), address + getpagesize(), getpagesize(), FALSE, VM_PROT_NONE);
   ASSERT_EQ(kr, KERN_SUCCESS) << MachErrorMessage(kr, "vm_protect");
 
   ProcessMemoryMac memory;
@@ -111,46 +111,46 @@ TEST(ProcessMemoryMac, ReadSelfUnmapped) {
 
   EXPECT_FALSE(memory.Read(address, kSize, &result[0]));
   EXPECT_FALSE(memory.Read(address + 1, kSize - 1, &result[0]));
-  EXPECT_FALSE(memory.Read(address + PAGE_SIZE, 1, &result[0]));
-  EXPECT_FALSE(memory.Read(address + PAGE_SIZE - 1, 2, &result[0]));
-  EXPECT_TRUE(memory.Read(address, PAGE_SIZE, &result[0]));
-  EXPECT_TRUE(memory.Read(address + PAGE_SIZE - 1, 1, &result[0]));
+  EXPECT_FALSE(memory.Read(address + getpagesize(), 1, &result[0]));
+  EXPECT_FALSE(memory.Read(address + getpagesize() - 1, 2, &result[0]));
+  EXPECT_TRUE(memory.Read(address, getpagesize(), &result[0]));
+  EXPECT_TRUE(memory.Read(address + getpagesize() - 1, 1, &result[0]));
 
   // Do the same thing with the ReadMapped() interface.
   std::unique_ptr<ProcessMemoryMac::MappedMemory> mapped;
   EXPECT_FALSE((mapped = memory.ReadMapped(address, kSize)));
   EXPECT_FALSE((mapped = memory.ReadMapped(address + 1, kSize - 1)));
-  EXPECT_FALSE((mapped = memory.ReadMapped(address + PAGE_SIZE, 1)));
-  EXPECT_FALSE((mapped = memory.ReadMapped(address + PAGE_SIZE - 1, 2)));
-  EXPECT_TRUE((mapped = memory.ReadMapped(address, PAGE_SIZE)));
-  EXPECT_TRUE((mapped = memory.ReadMapped(address + PAGE_SIZE - 1, 1)));
+  EXPECT_FALSE((mapped = memory.ReadMapped(address + getpagesize(), 1)));
+  EXPECT_FALSE((mapped = memory.ReadMapped(address + getpagesize() - 1, 2)));
+  EXPECT_TRUE((mapped = memory.ReadMapped(address, getpagesize())));
+  EXPECT_TRUE((mapped = memory.ReadMapped(address + getpagesize() - 1, 1)));
 
   // Repeat the test with an unmapped page instead of an unreadable one. This
   // portion of the test may be flaky in the presence of other threads, if
   // another thread maps something in the region that is deallocated here.
-  kr = vm_deallocate(mach_task_self(), address + PAGE_SIZE, PAGE_SIZE);
+  kr = vm_deallocate(mach_task_self(), address + getpagesize(), getpagesize());
   ASSERT_EQ(kr, KERN_SUCCESS) << MachErrorMessage(kr, "vm_deallocate");
-  vm_owner.reset(address, PAGE_SIZE);
+  vm_owner.reset(address, getpagesize());
 
   EXPECT_FALSE(memory.Read(address, kSize, &result[0]));
   EXPECT_FALSE(memory.Read(address + 1, kSize - 1, &result[0]));
-  EXPECT_FALSE(memory.Read(address + PAGE_SIZE, 1, &result[0]));
-  EXPECT_FALSE(memory.Read(address + PAGE_SIZE - 1, 2, &result[0]));
-  EXPECT_TRUE(memory.Read(address, PAGE_SIZE, &result[0]));
-  EXPECT_TRUE(memory.Read(address + PAGE_SIZE - 1, 1, &result[0]));
+  EXPECT_FALSE(memory.Read(address + getpagesize(), 1, &result[0]));
+  EXPECT_FALSE(memory.Read(address + getpagesize() - 1, 2, &result[0]));
+  EXPECT_TRUE(memory.Read(address, getpagesize(), &result[0]));
+  EXPECT_TRUE(memory.Read(address + getpagesize() - 1, 1, &result[0]));
 
   // Do the same thing with the ReadMapped() interface.
   EXPECT_FALSE((mapped = memory.ReadMapped(address, kSize)));
   EXPECT_FALSE((mapped = memory.ReadMapped(address + 1, kSize - 1)));
-  EXPECT_FALSE((mapped = memory.ReadMapped(address + PAGE_SIZE, 1)));
-  EXPECT_FALSE((mapped = memory.ReadMapped(address + PAGE_SIZE - 1, 2)));
-  EXPECT_TRUE((mapped = memory.ReadMapped(address, PAGE_SIZE)));
-  EXPECT_TRUE((mapped = memory.ReadMapped(address + PAGE_SIZE - 1, 1)));
+  EXPECT_FALSE((mapped = memory.ReadMapped(address + getpagesize(), 1)));
+  EXPECT_FALSE((mapped = memory.ReadMapped(address + getpagesize() - 1, 2)));
+  EXPECT_TRUE((mapped = memory.ReadMapped(address, getpagesize())));
+  EXPECT_TRUE((mapped = memory.ReadMapped(address + getpagesize() - 1, 1)));
 }
 
 TEST(ProcessMemoryMac, ReadCStringSelfUnmapped) {
   vm_address_t address = 0;
-  constexpr vm_size_t kSize = 2 * PAGE_SIZE;
+  constexpr vm_size_t kSize = 2 * getpagesize();
   kern_return_t kr =
       vm_allocate(mach_task_self(), &address, kSize, VM_FLAGS_ANYWHERE);
   ASSERT_EQ(kr, KERN_SUCCESS) << MachErrorMessage(kr, "vm_allocate");
@@ -164,7 +164,7 @@ TEST(ProcessMemoryMac, ReadCStringSelfUnmapped) {
   }
 
   kr = vm_protect(
-      mach_task_self(), address + PAGE_SIZE, PAGE_SIZE, FALSE, VM_PROT_NONE);
+      mach_task_self(), address + getpagesize(), getpagesize(), FALSE, VM_PROT_NONE);
   ASSERT_EQ(kr, KERN_SUCCESS) << MachErrorMessage(kr, "vm_protect");
 
   ProcessMemoryMac memory;
@@ -175,19 +175,19 @@ TEST(ProcessMemoryMac, ReadCStringSelfUnmapped) {
   // Make sure that if the string is NUL-terminated within the mapped memory
   // region, it can be read properly.
   char terminator_or_not = '\0';
-  std::swap(region[PAGE_SIZE - 1], terminator_or_not);
+  std::swap(region[getpagesize() - 1], terminator_or_not);
   ASSERT_TRUE(memory.ReadCString(address, &result));
   EXPECT_FALSE(result.empty());
-  EXPECT_EQ(result.size(), PAGE_SIZE - 1u);
+  EXPECT_EQ(result.size(), getpagesize() - 1u);
   EXPECT_EQ(result, region);
 
   // Repeat the test with an unmapped page instead of an unreadable one. This
   // portion of the test may be flaky in the presence of other threads, if
   // another thread maps something in the region that is deallocated here.
-  std::swap(region[PAGE_SIZE - 1], terminator_or_not);
-  kr = vm_deallocate(mach_task_self(), address + PAGE_SIZE, PAGE_SIZE);
+  std::swap(region[getpagesize() - 1], terminator_or_not);
+  kr = vm_deallocate(mach_task_self(), address + getpagesize(), getpagesize());
   ASSERT_EQ(kr, KERN_SUCCESS) << MachErrorMessage(kr, "vm_deallocate");
-  vm_owner.reset(address, PAGE_SIZE);
+  vm_owner.reset(address, getpagesize());
 
   EXPECT_FALSE(memory.ReadCString(address, &result));
 
@@ -195,10 +195,10 @@ TEST(ProcessMemoryMac, ReadCStringSelfUnmapped) {
   // sure that the result is actually filled in, because it already contains the
   // expected value from the tests above.
   result.clear();
-  std::swap(region[PAGE_SIZE - 1], terminator_or_not);
+  std::swap(region[getpagesize() - 1], terminator_or_not);
   ASSERT_TRUE(memory.ReadCString(address, &result));
   EXPECT_FALSE(result.empty());
-  EXPECT_EQ(result.size(), PAGE_SIZE - 1u);
+  EXPECT_EQ(result.size(), getpagesize() - 1u);
   EXPECT_EQ(result, region);
 }
 
@@ -260,6 +260,6 @@ TEST(ProcessMemoryMac, MappedMemoryDeallocates) {
   // single page. This makes sure that the whole mapped region winds up being
   // deallocated.
-  constexpr size_t kBigSize = 4 * PAGE_SIZE;
+  constexpr size_t kBigSize = 4 * getpagesize();
   std::unique_ptr<char[]> big_buffer(new char[kBigSize]);
   test_address = FromPointerCast<mach_vm_address_t>(&big_buffer[0]);
   ASSERT_TRUE((mapped = memory.ReadMapped(test_address, kBigSize)));
@@ -267,12 +267,12 @@ TEST(ProcessMemoryMac, MappedMemoryDeallocates) {
   mapped_address = reinterpret_cast<vm_address_t>(mapped->data());
   vm_address_t mapped_last_address = mapped_address + kBigSize - 1;
   EXPECT_TRUE(IsAddressMapped(mapped_address));
-  EXPECT_TRUE(IsAddressMapped(mapped_address + PAGE_SIZE));
+  EXPECT_TRUE(IsAddressMapped(mapped_address + getpagesize()));
   EXPECT_TRUE(IsAddressMapped(mapped_last_address));
 
   mapped.reset();
   EXPECT_FALSE(IsAddressMapped(mapped_address));
-  EXPECT_FALSE(IsAddressMapped(mapped_address + PAGE_SIZE));
+  EXPECT_FALSE(IsAddressMapped(mapped_address + getpagesize()));
   EXPECT_FALSE(IsAddressMapped(mapped_last_address));
 }
 
diff --git a/third_party/mach_override/mach_override.c b/third_party/mach_override/mach_override.c
index ca68c0e90e61..192df1f0a8dc 100644
--- a/third_party/mach_override/mach_override.c
+++ b/third_party/mach_override/mach_override.c
@@ -173,10 +173,10 @@ fixupInstructions(
 #if defined(__i386__) || defined(__x86_64__)
 mach_error_t makeIslandExecutable(void *address) {
 	mach_error_t err = err_none;
-    uintptr_t page = (uintptr_t)address & ~(uintptr_t)(PAGE_SIZE - 1);
+    uintptr_t page = (uintptr_t)address & ~(uintptr_t)(getpagesize() - 1);
     int e = err_none;
-    e |= mprotect((void *)page, PAGE_SIZE, PROT_EXEC | PROT_READ);
-    e |= msync((void *)page, PAGE_SIZE, MS_INVALIDATE );
+    e |= mprotect((void *)page, getpagesize(), PROT_EXEC | PROT_READ);
+    e |= msync((void *)page, getpagesize(), MS_INVALIDATE );
     if (e) {
         err = err_cannot_override;
     }
@@ -409,25 +409,25 @@ allocateBranchIsland(
 	mach_error_t	err = err_none;
 	
 	if( allocateHigh ) {
-		assert( sizeof( BranchIsland ) <= PAGE_SIZE );
+		assert( sizeof( BranchIsland ) <= getpagesize() );
 		vm_address_t page = 0;
 #if defined(__i386__)
 		OSAtomicAdd64(1, &g_mach_override_allocation_attempts);
-		err = vm_allocate( mach_task_self(), &page, PAGE_SIZE, VM_FLAGS_ANYWHERE );
+		err = vm_allocate( mach_task_self(), &page, getpagesize(), VM_FLAGS_ANYWHERE );
 		if( err == err_none )
 			*island = (BranchIsland*) page;
 #else
 
 #if defined(__ppc__) || defined(__POWERPC__)
 		vm_address_t first = 0xfeffffff;
-		vm_address_t last = 0xfe000000 + PAGE_SIZE;
+		vm_address_t last = 0xfe000000 + getpagesize();
 #elif defined(__x86_64__)
 		// This logic is more complex due to the 32-bit limit of the jump out
 		// of the original function. Once that limitation is removed, we can
 		// use vm_allocate with VM_FLAGS_ANYWHERE as in the i386 code above.
-		const uint64_t kPageMask = ~(PAGE_SIZE - 1);
+		const uint64_t kPageMask = ~(getpagesize() - 1);
 		vm_address_t first = (uint64_t)originalFunctionAddress - kMaxJumpOffset;
-		first = (first & kPageMask) + PAGE_SIZE; // Align up to the next page start
+		first = (first & kPageMask) + getpagesize(); // Align up to the next page start
 		if (first > (uint64_t)originalFunctionAddress) first = 0;
 		vm_address_t last = (uint64_t)originalFunctionAddress + kMaxJumpOffset;
 		if (last < (uint64_t)originalFunctionAddress) last = ULONG_MAX;
@@ -440,7 +440,7 @@ allocateBranchIsland(
 		while( !err && !allocated && page < last ) {
 
 			OSAtomicAdd64(1, &g_mach_override_allocation_attempts);
-			err = vm_allocate( task_self, &page, PAGE_SIZE, 0 );
+			err = vm_allocate( task_self, &page, getpagesize(), 0 );
 			if( err == err_none )
 				allocated = 1;
 			else if( err == KERN_NO_SPACE || err == KERN_INVALID_ADDRESS) {
@@ -459,7 +459,7 @@ allocateBranchIsland(
 				else
 					break;
 #else
-				page += PAGE_SIZE;
+				page += getpagesize();
 #endif
 				err = err_none;
 			}
@@ -502,8 +502,8 @@ freeBranchIsland(
 	mach_error_t	err = err_none;
 	
 	if( island->allocatedHigh ) {
-		assert( sizeof( BranchIsland ) <= PAGE_SIZE );
-		err = vm_deallocate(mach_task_self(), (vm_address_t) island, PAGE_SIZE );
+		assert( sizeof( BranchIsland ) <= getpagesize() );
+		err = vm_deallocate(mach_task_self(), (vm_address_t) island, getpagesize() );
 	} else {
 		free( island );
 	}
